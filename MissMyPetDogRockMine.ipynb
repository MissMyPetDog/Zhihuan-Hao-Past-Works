{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use Sonar dataset from sklearn.datasets, which contains sonar signals for classifying objects as either \"rock\" or \"mine.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "sonar = fetch_openml(name=\"sonar\", version=1)\n",
        "\n",
        "X = sonar.data  # Features\n",
        "y = sonar.target  # Target (rock or mine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a) Begin by creating a training and testing datasest from the dataset, with a 80-20 ratio, and random_state=1. **1 pt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b) Train a KNN classifier on the training set to classify sonar signals as either \"Rock\" or \"Mine.\" Use cross-validation to find an appropriate value of K. Evaluate and print the model's performance on the testing set using accuracy. **-- 9 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation accuracies for different k values:\n",
            "K = 1: Mean Accuracy = 0.8137\n",
            "K = 2: Mean Accuracy = 0.7891\n",
            "K = 3: Mean Accuracy = 0.8012\n",
            "K = 4: Mean Accuracy = 0.7652\n",
            "K = 5: Mean Accuracy = 0.7717\n",
            "K = 6: Mean Accuracy = 0.7234\n",
            "K = 7: Mean Accuracy = 0.7358\n",
            "K = 8: Mean Accuracy = 0.6873\n",
            "K = 9: Mean Accuracy = 0.6875\n",
            "K = 10: Mean Accuracy = 0.6934\n",
            "K = 11: Mean Accuracy = 0.6811\n",
            "K = 12: Mean Accuracy = 0.6451\n",
            "K = 13: Mean Accuracy = 0.6512\n",
            "K = 14: Mean Accuracy = 0.6148\n",
            "K = 15: Mean Accuracy = 0.6390\n",
            "K = 16: Mean Accuracy = 0.6209\n",
            "K = 17: Mean Accuracy = 0.6449\n",
            "K = 18: Mean Accuracy = 0.6267\n",
            "K = 19: Mean Accuracy = 0.6330\n",
            "K = 20: Mean Accuracy = 0.6269\n",
            "\n",
            "Best k value based on cross-validation: 1 with accuracy 0.8137\n",
            "\n",
            "Accuracy on the test set using the best k (1): 0.7619\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the range of k values\n",
        "k_values = list(range(1, 21))\n",
        "\n",
        "# Dictionary to store cross-validation accuracies for each k\n",
        "cv_accuracies = {}\n",
        "\n",
        "# Loop through different k values and perform cross-validation\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    cv_scores = cross_val_score(knn, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
        "    cv_accuracies[k] = cv_scores.mean()\n",
        "\n",
        "# Find the best k based on cross-validation accuracy\n",
        "best_k = max(cv_accuracies, key=cv_accuracies.get)\n",
        "best_accuracy = cv_accuracies[best_k]\n",
        "\n",
        "print(\"Cross-validation accuracies for different k values:\")\n",
        "for k, accuracy in cv_accuracies.items():\n",
        "    print(f\"K = {k}: Mean Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nBest k value based on cross-validation: {best_k} with accuracy {best_accuracy:.4f}\")\n",
        "\n",
        "# Training the KNN model with the best k on the entire training set\n",
        "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "best_knn.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set using the best model\n",
        "y_pred = best_knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy on the test set using the best k ({best_k}): {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.0139, 0.0222, 0.0089, ..., 0.0059, 0.0039, 0.0048],\n",
              "       [0.0411, 0.0277, 0.0604, ..., 0.005 , 0.0085, 0.0044],\n",
              "       [0.0731, 0.1249, 0.1665, ..., 0.0194, 0.0207, 0.0057],\n",
              "       ...,\n",
              "       [0.0208, 0.0186, 0.0131, ..., 0.0019, 0.0049, 0.0023],\n",
              "       [0.0412, 0.1135, 0.0518, ..., 0.0225, 0.0098, 0.0085],\n",
              "       [0.0333, 0.0221, 0.027 , ..., 0.0132, 0.0051, 0.0041]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'data': array([[0.02  , 0.0371, 0.0428, ..., 0.0084, 0.009 , 0.0032],\n",
              "        [0.0453, 0.0523, 0.0843, ..., 0.0049, 0.0052, 0.0044],\n",
              "        [0.0262, 0.0582, 0.1099, ..., 0.0164, 0.0095, 0.0078],\n",
              "        ...,\n",
              "        [0.0522, 0.0437, 0.018 , ..., 0.0138, 0.0077, 0.0031],\n",
              "        [0.0303, 0.0353, 0.049 , ..., 0.0079, 0.0036, 0.0048],\n",
              "        [0.026 , 0.0363, 0.0136, ..., 0.0036, 0.0061, 0.0115]]),\n",
              " 'target': array(['Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock',\n",
              "        'Rock', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine',\n",
              "        'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine', 'Mine'],\n",
              "       dtype=object),\n",
              " 'frame': None,\n",
              " 'categories': {},\n",
              " 'feature_names': ['attribute_1',\n",
              "  'attribute_2',\n",
              "  'attribute_3',\n",
              "  'attribute_4',\n",
              "  'attribute_5',\n",
              "  'attribute_6',\n",
              "  'attribute_7',\n",
              "  'attribute_8',\n",
              "  'attribute_9',\n",
              "  'attribute_10',\n",
              "  'attribute_11',\n",
              "  'attribute_12',\n",
              "  'attribute_13',\n",
              "  'attribute_14',\n",
              "  'attribute_15',\n",
              "  'attribute_16',\n",
              "  'attribute_17',\n",
              "  'attribute_18',\n",
              "  'attribute_19',\n",
              "  'attribute_20',\n",
              "  'attribute_21',\n",
              "  'attribute_22',\n",
              "  'attribute_23',\n",
              "  'attribute_24',\n",
              "  'attribute_25',\n",
              "  'attribute_26',\n",
              "  'attribute_27',\n",
              "  'attribute_28',\n",
              "  'attribute_29',\n",
              "  'attribute_30',\n",
              "  'attribute_31',\n",
              "  'attribute_32',\n",
              "  'attribute_33',\n",
              "  'attribute_34',\n",
              "  'attribute_35',\n",
              "  'attribute_36',\n",
              "  'attribute_37',\n",
              "  'attribute_38',\n",
              "  'attribute_39',\n",
              "  'attribute_40',\n",
              "  'attribute_41',\n",
              "  'attribute_42',\n",
              "  'attribute_43',\n",
              "  'attribute_44',\n",
              "  'attribute_45',\n",
              "  'attribute_46',\n",
              "  'attribute_47',\n",
              "  'attribute_48',\n",
              "  'attribute_49',\n",
              "  'attribute_50',\n",
              "  'attribute_51',\n",
              "  'attribute_52',\n",
              "  'attribute_53',\n",
              "  'attribute_54',\n",
              "  'attribute_55',\n",
              "  'attribute_56',\n",
              "  'attribute_57',\n",
              "  'attribute_58',\n",
              "  'attribute_59',\n",
              "  'attribute_60'],\n",
              " 'target_names': ['Class'],\n",
              " 'DESCR': '**Author**:   \\n**Source**: Unknown -   \\n**Please cite**:   \\n\\nNAME: Sonar, Mines vs. Rocks\\n \\n SUMMARY: This is the data set used by Gorman and Sejnowski in their study\\n of the classification of sonar signals using a neural network [1].  The\\n task is to train a network to discriminate between sonar signals bounced\\n off a metal cylinder and those bounced off a roughly cylindrical rock.\\n \\n SOURCE: The data set was contributed to the benchmark collection by Terry\\n Sejnowski, now at the Salk Institute and the University of California at\\n San Deigo.  The data set was developed in collaboration with R. Paul\\n Gorman of Allied-Signal Aerospace Technology Center.\\n \\n MAINTAINER: Scott E. Fahlman\\n \\n PROBLEM DESCRIPTION:\\n \\n The file \"sonar.mines\" contains 111 patterns obtained by bouncing sonar\\n signals off a metal cylinder at various angles and under various\\n conditions.  The file \"sonar.rocks\" contains 97 patterns obtained from\\n rocks under similar conditions.  The transmitted sonar signal is a\\n frequency-modulated chirp, rising in frequency.  The data set contains\\n signals obtained from a variety of different aspect angles, spanning 90\\n degrees for the cylinder and 180 degrees for the rock.\\n \\n Each pattern is a set of 60 numbers in the range 0.0 to 1.0.  Each number\\n represents the energy within a particular frequency band, integrated over\\n a certain period of time.  The integration aperture for higher frequencies\\n occur later in time, since these frequencies are transmitted later during\\n the chirp.\\n \\n The label associated with each record contains the letter \"R\" if the object\\n is a rock and \"M\" if it is a mine (metal cylinder).  The numbers in the\\n labels are in increasing order of aspect angle, but they do not encode the\\n angle directly.\\n \\n METHODOLOGY: \\n \\n This data set can be used in a number of different ways to test learning\\n speed, quality of ultimate learning, ability to generalize, or combinations\\n of these factors.\\n \\n In [1], Gorman and Sejnowski report two series of experiments: an\\n \"aspect-angle independent\" series, in which the whole data set is used\\n without controlling for aspect angle, and an \"aspect-angle dependent\"\\n series in which the training and testing sets were carefully controlled to\\n ensure that each set contained cases from each aspect angle in\\n appropriate proportions.\\n \\n For the aspect-angle independent experiments the combined set of 208 cases\\n is divided randomly into 13 disjoint sets with 16 cases in each.  For each\\n experiment, 12 of these sets are used as training data, while the 13th is\\n reserved for testing.  The experiment is repeated 13 times so that every\\n case appears once as part of a test set.  The reported performance is an\\n average over the entire set of 13 different test sets, each run 10 times.\\n \\n It was observed that this random division of the sample set led to rather\\n uneven performance.  A few of the splits gave poor results, presumably\\n because the test set contains some samples from aspect angles that are\\n under-represented in the corresponding training set.  This motivated Gorman\\n and Sejnowski to devise a different set of experiments in which an attempt\\n was made to balance the training and test sets so that each would have a\\n representative number of samples from all aspect angles.  Since detailed\\n aspect angle information was not present in the data base of samples, the\\n 208 samples were first divided into clusters, using a 60-dimensional\\n Euclidian metric; each of these clusters was then divided between the\\n 104-member training set and the 104-member test set.  \\n \\n The actual training and testing samples used for the \"aspect angle\\n dependent\" experiments are marked in the data files.  The reported\\n performance is an average over 10 runs with this single division of the\\n data set.\\n \\n A standard back-propagation network was used for all experiments.  The\\n network had 60 inputs and 2 output units, one indicating a cylinder and the\\n other a rock.  Experiments were run with no hidden units (direct\\n connections from each input to each output) and with a single hidden layer\\n with 2, 3, 6, 12, or 24 units.  Each network was trained by 300 epochs over\\n the entire training set.\\n \\n The weight-update formulas used in this study were slightly different from\\n the standard form.  A learning rate of 2.0 and momentum of 0.0 was used.\\n Errors less than 0.2 were treated as zero.  Initial weights were uniform\\n random values in the range -0.3 to +0.3.\\n \\n RESULTS: \\n \\n For the angle independent experiments, Gorman and Sejnowski report the\\n following results for networks with different numbers of hidden units:\\n \\n Hidden\\t% Right on\\tStd.\\t% Right on\\tStd.\\n Units\\tTraining set\\tDev.\\tTest Set\\tDev.\\n ------\\t------------\\t----\\t----------\\t----\\n 0\\t89.4\\t\\t2.1\\t77.1\\t\\t8.3\\n 2\\t96.5\\t\\t0.7\\t81.9\\t\\t6.2\\n 3\\t98.8\\t\\t0.4\\t82.0\\t\\t7.3\\n 6\\t99.7\\t\\t0.2\\t83.5\\t\\t5.6\\n 12\\t99.8\\t\\t0.1\\t84.7\\t\\t5.7\\n 24\\t99.8\\t\\t0.1\\t84.5\\t\\t5.7\\n \\n For the angle-dependent experiments Gorman and Sejnowski report the\\n following results:\\n \\n Hidden\\t% Right on\\tStd.\\t% Right on\\tStd.\\n Units\\tTraining set\\tDev.\\tTest Set\\tDev.\\n ------\\t------------\\t----\\t----------\\t----\\n 0\\t79.3\\t\\t3.4\\t73.1\\t\\t4.8\\n 2\\t96.2\\t\\t2.2\\t85.7\\t\\t6.3\\n 3\\t98.1\\t\\t1.5\\t87.6\\t\\t3.0\\n 6\\t99.4\\t\\t0.9\\t89.3\\t\\t2.4\\n 12\\t99.8\\t\\t0.6\\t90.4\\t\\t1.8\\n 24     100.0\\t\\t0.0\\t89.2\\t\\t1.4\\n \\n Not surprisingly, the network\\'s performance on the test set was somewhat\\n better when the aspect angles in the training and test sets were balanced.\\n \\n Gorman and Sejnowski further report that a nearest neighbor classifier on\\n the same data gave an 82.7% probability of correct classification.\\n \\n Three trained human subjects were each tested on 100 signals, chosen at\\n random from the set of 208 returns used to create this data set.  Their\\n responses ranged between 88% and 97% correct.  However, they may have been\\n using information from the raw sonar signal that is not preserved in the\\n processed data sets presented here.\\n \\n REFERENCES: \\n \\n 1. Gorman, R. P., and Sejnowski, T. J. (1988).  \"Analysis of Hidden Units\\n in a Layered Network Trained to Classify Sonar Targets\" in Neural Networks,\\n Vol. 1, pp. 75-89.\\n\\n\\n\\n\\n Relabeled values in attribute \\'Class\\'\\n    From: R                       To: Rock                \\n    From: M                       To: Mine\\n\\nDownloaded from openml.org.',\n",
              " 'details': {'id': '40',\n",
              "  'name': 'sonar',\n",
              "  'version': '1',\n",
              "  'description_version': '1',\n",
              "  'format': 'ARFF',\n",
              "  'creator': 'R. Paul Gorman',\n",
              "  'collection_date': '1988-10-30',\n",
              "  'upload_date': '2014-04-06T23:22:24',\n",
              "  'language': 'English',\n",
              "  'licence': 'Public',\n",
              "  'url': 'https://api.openml.org/data/v1/download/40/sonar.arff',\n",
              "  'parquet_url': 'http://openml1.win.tue.nl/dataset40/dataset_40.pq',\n",
              "  'file_id': '40',\n",
              "  'default_target_attribute': 'Class',\n",
              "  'version_label': '1',\n",
              "  'citation': 'https://archive.ics.uci.edu/ml/citation_policy.html',\n",
              "  'tag': ['mythbusting_1',\n",
              "   'study_1',\n",
              "   'study_123',\n",
              "   'study_15',\n",
              "   'study_20',\n",
              "   'study_29',\n",
              "   'study_30',\n",
              "   'study_41',\n",
              "   'study_50',\n",
              "   'study_52',\n",
              "   'study_7',\n",
              "   'study_88',\n",
              "   'uci'],\n",
              "  'visibility': 'public',\n",
              "  'original_data_url': 'https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)',\n",
              "  'paper_url': 'https://www.sciencedirect.com/science/article/abs/pii/0893608088900238',\n",
              "  'minio_url': 'http://openml1.win.tue.nl/dataset40/dataset_40.pq',\n",
              "  'status': 'active',\n",
              "  'processing_date': '2020-11-20 19:08:05',\n",
              "  'md5_checksum': '3ab630fbbfe25ab48b9bb47ce5759203'},\n",
              " 'url': 'https://www.openml.org/d/40'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sonar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.02  , 0.0371, 0.0428, ..., 0.0084, 0.009 , 0.0032],\n",
              "       [0.0453, 0.0523, 0.0843, ..., 0.0049, 0.0052, 0.0044],\n",
              "       [0.0262, 0.0582, 0.1099, ..., 0.0164, 0.0095, 0.0078],\n",
              "       ...,\n",
              "       [0.0522, 0.0437, 0.018 , ..., 0.0138, 0.0077, 0.0031],\n",
              "       [0.0303, 0.0353, 0.049 , ..., 0.0079, 0.0036, 0.0048],\n",
              "       [0.026 , 0.0363, 0.0136, ..., 0.0036, 0.0061, 0.0115]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c) Using any combination of the classification tools we've discussed in class:\n",
        "\n",
        "- KNN\n",
        "- Naive Bayes\n",
        "- SVM\n",
        "- Decision Tree (including Random Forests)\n",
        "- Ensemble Methods (AdaBoost, Bagging)\n",
        "\n",
        "You may also use feature extraction tools like PCA. Train and tune a model on the training set and evaluate its performance on the test set using accuracy. **-- 30 points**\n",
        "\n",
        " * accuracy > .95 **-- 30 points**\n",
        " * accuracy between 0.94 and 0.95 **-- 25 points**\n",
        " * accuracy between 0.92 and 0.94 **-- 20 points**\n",
        " * accuracy between 0.9 and 0.92 **-- 15 points**\n",
        " * accuracy between 0.85 and 0.9 **-- 10 points**\n",
        " * accuracy between 0.8 and 0.85 **-- 7 points**\n",
        " * accuracy between 0.7 and 0.8 **-- 5 points**\n",
        " * accuracy < 0.7 **-- 3 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "import xgboost as xgboost\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 0.98)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.08238329, -0.8415542 , -0.37129052, ...,  0.01706236,\n",
              "         0.08270075, -0.02232914],\n",
              "       [ 1.31114804,  0.04786938,  0.35541227, ..., -0.00252458,\n",
              "        -0.12711949,  0.02057906],\n",
              "       [ 0.18601859,  0.20724731,  0.68373819, ...,  0.04693734,\n",
              "        -0.11561944,  0.17685122],\n",
              "       ...,\n",
              "       [ 0.03711126, -0.94687492, -0.41512858, ..., -0.15250896,\n",
              "        -0.06377815,  0.00890625],\n",
              "       [ 0.76279092,  0.54315745,  1.44873934, ..., -0.12771044,\n",
              "         0.08777093,  0.04116873],\n",
              "       [ 0.60750661, -1.05395462, -0.31667639, ..., -0.0421534 ,\n",
              "        -0.06452961, -0.0213803 ]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The one with the highest accuracy has been put bewlow this comment, which is an accuracy of 0.9524"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on the test set using the knn with k = 1: 0.9524\n"
          ]
        }
      ],
      "source": [
        "# Training the KNN model with the best k on the entire training set\n",
        "best_k = 1\n",
        "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "best_knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predicting on the test set using the best model\n",
        "y_pred = best_knn.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy on the test set using the knn with k = {best_k}: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with KNN Classifier: 0.6905\n"
          ]
        }
      ],
      "source": [
        "# Create a KNN base classifier\n",
        "base_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Create Bagging classifier with KNN as base estimator\n",
        "bagging_knn = BaggingClassifier(base_estimator=base_knn, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging with KNN Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned Naive Bayes Classifier: 0.6667\n",
            "Best alpha value: 0.1\n"
          ]
        }
      ],
      "source": [
        "# Create a Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Define the grid of parameters to search\n",
        "param_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]}\n",
        "\n",
        "# Perform GridSearchCV to tune the 'alpha' parameter\n",
        "grid_search = GridSearchCV(nb_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameter value\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Train the model using the best alpha value\n",
        "best_nb_classifier = MultinomialNB(alpha=best_alpha)\n",
        "best_nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_nb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Tuned Naive Bayes Classifier: {accuracy:.4f}\")\n",
        "print(f\"Best alpha value: {best_alpha}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.7143\n"
          ]
        }
      ],
      "source": [
        "# Create a base classifier (e.g., Decision Tree)\n",
        "base_classifier_nb = MultinomialNB()\n",
        "\n",
        "# Create AdaBoost classifier with a base estimator and set random_state\n",
        "adaboost_classifier = AdaBoostClassifier(base_estimator = base_classifier_nb, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with KNN Classifier: 0.6667\n"
          ]
        }
      ],
      "source": [
        "# Create Bagging classifier with nb as base estimator\n",
        "bagging_nb = BaggingClassifier(base_estimator=base_classifier_nb, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_nb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_nb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging with KNN Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned SVM Classifier: 0.8095\n",
            "Best C value: 10\n",
            "Best gamma value: 0.1\n"
          ]
        }
      ],
      "source": [
        "# Create an SVM classifier\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Define the grid of parameters to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Penalty parameter C\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient for 'rbf'\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid']  # Kernel type\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameter values\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_gamma = grid_search.best_params_['gamma']\n",
        "best_kernel = grid_search.best_params_['kernel']\n",
        "# Train the model using the best parameters\n",
        "best_svm_classifier = SVC(C=best_C, gamma=best_gamma, kernel = 'poly')\n",
        "best_svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Tuned SVM Classifier: {accuracy:.4f}\")\n",
        "print(f\"Best C value: {best_C}\")\n",
        "print(f\"Best gamma value: {best_gamma}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned SVM Classifier: 0.5238\n",
            "Best C value: 10\n",
            "Best gamma value: 0.1\n"
          ]
        }
      ],
      "source": [
        "# Create an SVM classifier\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Define the grid of parameters to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Penalty parameter C\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient for 'rbf'\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid']  # Kernel type\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_pca, y_train)\n",
        "\n",
        "# Get the best parameter values\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_gamma = grid_search.best_params_['gamma']\n",
        "best_kernel = grid_search.best_params_['kernel']\n",
        "# Train the model using the best parameters\n",
        "best_svm_classifier = SVC(C=best_C, gamma=best_gamma, kernel = 'poly')\n",
        "best_svm_classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_svm_classifier.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Tuned SVM Classifier: {accuracy:.4f}\")\n",
        "print(f\"Best C value: {best_C}\")\n",
        "print(f\"Best gamma value: {best_gamma}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.7381\n"
          ]
        }
      ],
      "source": [
        "# Create a base classifier (e.g., Decision Tree)\n",
        "base_classifier_svm = SVC(probability= True)\n",
        "\n",
        "# Create AdaBoost classifier with a base estimator and set random_state\n",
        "adaboost_classifier = AdaBoostClassifier(base_estimator = base_classifier_svm, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.8571\n"
          ]
        }
      ],
      "source": [
        "# Create a base classifier (e.g., Decision Tree)\n",
        "base_classifier_svm = SVC(probability= True)\n",
        "\n",
        "# Create AdaBoost classifier with a base estimator and set random_state\n",
        "adaboost_classifier = AdaBoostClassifier(base_estimator = base_classifier_svm, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost_classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with KNN Classifier: 0.8333\n"
          ]
        }
      ],
      "source": [
        "# Create Bagging classifier with nb as base estimator\n",
        "bagging_svm = BaggingClassifier(base_estimator=base_classifier_svm, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging with KNN Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with KNN Classifier: 0.8571\n"
          ]
        }
      ],
      "source": [
        "# Create Bagging classifier with nb as base estimator\n",
        "bagging_svm = BaggingClassifier(base_estimator=base_classifier_svm, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_svm.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_svm.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging with KNN Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned Decision Tree Classifier: 0.6905\n",
            "Best criterion: entropy\n",
            "Best max_depth: None\n",
            "Best min_samples_split: 5\n"
          ]
        }
      ],
      "source": [
        "tree_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Define the grid of parameters to search\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
        "    'max_depth': [None, 5, 10, 15, 20, 25],  # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split a node\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(tree_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameter values\n",
        "best_criterion = grid_search.best_params_['criterion']\n",
        "best_max_depth = grid_search.best_params_['max_depth']\n",
        "best_min_samples_split = grid_search.best_params_['min_samples_split']\n",
        "\n",
        "# Train the model using the best parameters\n",
        "best_tree_classifier = DecisionTreeClassifier(criterion = best_criterion, \n",
        "                                              max_depth=best_max_depth, \n",
        "                                              min_samples_split = best_min_samples_split)\n",
        "best_tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_tree_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Tuned Decision Tree Classifier: {accuracy:.4f}\")\n",
        "print(f\"Best criterion: {best_criterion}\")\n",
        "print(f\"Best max_depth: {best_max_depth}\")\n",
        "print(f\"Best min_samples_split: {best_min_samples_split}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned Decision Tree Classifier: 0.6667\n",
            "Best criterion: entropy\n",
            "Best max_depth: 5\n",
            "Best min_samples_split: 10\n"
          ]
        }
      ],
      "source": [
        "tree_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Define the grid of parameters to search\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
        "    'max_depth': [None, 5, 10, 15, 20, 25],  # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split a node\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(tree_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_pca, y_train)\n",
        "\n",
        "# Get the best parameter values\n",
        "best_criterion = grid_search.best_params_['criterion']\n",
        "best_max_depth = grid_search.best_params_['max_depth']\n",
        "best_min_samples_split = grid_search.best_params_['min_samples_split']\n",
        "\n",
        "# Train the model using the best parameters\n",
        "best_tree_classifier = DecisionTreeClassifier(criterion = best_criterion, \n",
        "                                              max_depth=best_max_depth, \n",
        "                                              min_samples_split = best_min_samples_split)\n",
        "best_tree_classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_tree_classifier.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Tuned Decision Tree Classifier: {accuracy:.4f}\")\n",
        "print(f\"Best criterion: {best_criterion}\")\n",
        "print(f\"Best max_depth: {best_max_depth}\")\n",
        "print(f\"Best min_samples_split: {best_min_samples_split}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.8571\n"
          ]
        }
      ],
      "source": [
        "# Create a base classifier (e.g., Decision Tree)\n",
        "base_classifier_dc = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# Create AdaBoost classifier with a base estimator and set random_state\n",
        "adaboost_classifier = AdaBoostClassifier(base_estimator=base_classifier_dc, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.6667\n"
          ]
        }
      ],
      "source": [
        "# Create a base classifier (e.g., Decision Tree)\n",
        "base_classifier_dc = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# Create AdaBoost classifier with a base estimator and set random_state\n",
        "adaboost_classifier = AdaBoostClassifier(base_estimator=base_classifier_dc, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost_classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with bagging Classifier: 0.7381\n"
          ]
        }
      ],
      "source": [
        "# Create Bagging classifier with nb as base estimator\n",
        "bagging_dc = BaggingClassifier(base_estimator=base_classifier_dc, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_dc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_dc.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging with bagging Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with bGGING Classifier: 0.7619\n"
          ]
        }
      ],
      "source": [
        "# Create Bagging classifier with nb as base estimator\n",
        "bagging_dc = BaggingClassifier(base_estimator=base_classifier_dc, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_dc.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_dc.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging with bGGING Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned Random Forest Classifier: 0.8095\n",
            "Best n_estimators: 100\n",
            "Best criterion: entropy\n",
            "Best max_depth: None\n",
            "Best min_samples_split: 5\n"
          ]
        }
      ],
      "source": [
        "# Define the grid of parameters to search\n",
        "\n",
        "forest_classifier = RandomForestClassifier(random_state = 42)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
        "    'max_depth': [None, 5, 10, 15],  # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split a node\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(forest_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameter values\n",
        "best_n_estimators = grid_search.best_params_['n_estimators']\n",
        "best_criterion = grid_search.best_params_['criterion']\n",
        "best_max_depth = grid_search.best_params_['max_depth']\n",
        "best_min_samples_split = grid_search.best_params_['min_samples_split']\n",
        "\n",
        "# Train the model using the best parameters\n",
        "best_forest_classifier = RandomForestClassifier(n_estimators=best_n_estimators, criterion=best_criterion,\n",
        "                                                max_depth=best_max_depth,\n",
        "                                                min_samples_split=best_min_samples_split)\n",
        "best_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_forest_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Tuned Random Forest Classifier: {accuracy:.4f}\")\n",
        "print(f\"Best n_estimators: {best_n_estimators}\")\n",
        "print(f\"Best criterion: {best_criterion}\")\n",
        "print(f\"Best max_depth: {best_max_depth}\")\n",
        "print(f\"Best min_samples_split: {best_min_samples_split}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned Random Forest Classifier: 0.7619\n",
            "Best n_estimators: 200\n",
            "Best criterion: gini\n",
            "Best max_depth: None\n",
            "Best min_samples_split: 2\n"
          ]
        }
      ],
      "source": [
        "# Define the grid of parameters to search\n",
        "\n",
        "forest_classifier = RandomForestClassifier(random_state = 42)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
        "    'max_depth': [None, 5, 10, 15],  # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split a node\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(forest_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_pca, y_train)\n",
        "\n",
        "# Get the best parameter values\n",
        "best_n_estimators = grid_search.best_params_['n_estimators']\n",
        "best_criterion = grid_search.best_params_['criterion']\n",
        "best_max_depth = grid_search.best_params_['max_depth']\n",
        "best_min_samples_split = grid_search.best_params_['min_samples_split']\n",
        "\n",
        "# Train the model using the best parameters\n",
        "best_forest_classifier = RandomForestClassifier(n_estimators=best_n_estimators, criterion=best_criterion,\n",
        "                                                max_depth=best_max_depth,\n",
        "                                                min_samples_split=best_min_samples_split)\n",
        "best_forest_classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_forest_classifier.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Tuned Random Forest Classifier: {accuracy:.4f}\")\n",
        "print(f\"Best n_estimators: {best_n_estimators}\")\n",
        "print(f\"Best criterion: {best_criterion}\")\n",
        "print(f\"Best max_depth: {best_max_depth}\")\n",
        "print(f\"Best min_samples_split: {best_min_samples_split}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.8333\n"
          ]
        }
      ],
      "source": [
        "# Create a base classifier (e.g., Decision Tree)\n",
        "base_classifier_rfm = RandomForestClassifier()\n",
        "\n",
        "# Create AdaBoost classifier with a base estimator and set random_state\n",
        "adaboost_classifier = AdaBoostClassifier(base_estimator=base_classifier_rfm, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.7857\n"
          ]
        }
      ],
      "source": [
        "# Create a base classifier (e.g., Decision Tree)\n",
        "base_classifier_rfm = RandomForestClassifier()\n",
        "\n",
        "# Create AdaBoost classifier with a base estimator and set random_state\n",
        "adaboost_classifier = AdaBoostClassifier(base_estimator=base_classifier_rfm, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost_classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with rfm Classifier: 0.7381\n"
          ]
        }
      ],
      "source": [
        "bagging_rfm = BaggingClassifier(base_estimator = base_classifier_rfm, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_rfm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_rfm.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging with rfm Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with rfm Classifier: 0.7619\n"
          ]
        }
      ],
      "source": [
        "bagging_rfm = BaggingClassifier(base_estimator = base_classifier_rfm, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_rfm.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_rfm.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging with rfm Classifier: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus (15pts)\n",
        "\n",
        "In this bonus we will implement 1-dimensional GMM clustering algorithm from scratch. A GMM distribution is composed of `k` components, each characterized by:\n",
        "\n",
        "1. A mixture proportion\n",
        "2. A mean for its Normal Distribution\n",
        "3. A variance for its Normal Distribution\n",
        "\n",
        "So, to generate a dataset that follows a GMM distrbution we need a list of those parameters. In this exercise we will use a class called `Component` to capture the parameters for a given component. And a GMM will be a list of `Component`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Component:\n",
        "    def __init__(self, mixture_prop, mean, variance):\n",
        "        self.mixture_prop = mixture_prop\n",
        "        self.mean = mean\n",
        "        self.variance = variance\n",
        "\n",
        "example_gmm = [Component(.5, 5, 1), Component(.5, 8, 1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a) Complete the function below to validate and generate a dataset following a GMM distribution, given a specified set of GMM parameters as above and a size. You may only use the methods already imported in the cell. (10pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'ellipsis' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-33-2fa2d7b9b9dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# test your code: this should return a list of 10 numbers similar to worksheet 8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_gmm_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_gmm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-33-2fa2d7b9b9dd>\u001b[0m in \u001b[0;36mgenerate_gmm_dataset\u001b[1;34m(gmm_params, size)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mcomp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_random_component\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgmm_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: 'ellipsis' object is not iterable"
          ]
        }
      ],
      "source": [
        "from numpy.random import normal, uniform\n",
        "\n",
        "def generate_gmm_dataset(gmm_params, size):\n",
        "    if not is_valid_gmm(gmm_params):\n",
        "        raise ValueError(\"GMM parameters are invalid\")\n",
        "    \n",
        "    dataset = []\n",
        "    for _ in range(size):\n",
        "        comp = get_random_component(gmm_params)\n",
        "        dataset += ...\n",
        "    return dataset\n",
        "\n",
        "def is_valid_gmm(gmm_params):\n",
        "    '''\n",
        "        Checks that the sum of the mixture\n",
        "        proportions is 1\n",
        "    '''\n",
        "    return True\n",
        "\n",
        "def get_random_component(gmm_params):\n",
        "    '''\n",
        "        returns component with prob\n",
        "        proportional to mixture_prop\n",
        "    '''\n",
        "    ...\n",
        "    return \n",
        "\n",
        "# test your code: this should return a list of 10 numbers similar to worksheet 8\n",
        "data = generate_gmm_dataset(example_gmm, 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b) Finish the implementation below of the Expectation-Maximization Algorithm. Only use methods that have been imported in the cell. Visualize the output of your code by plotting the original mixture distribution curves and the ones learned by the EM algorithm. (15pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "from numpy import array, argmax\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def gmm_init(k, dataset):\n",
        "    kmeans = KMeans(k, init='k-means++').fit(X=array(dataset).reshape(-1, 1))\n",
        "    gmm_params = []\n",
        "    ...\n",
        "    return gmm_params\n",
        "\n",
        "\n",
        "def compute_gmm(k, dataset, probs):\n",
        "    '''\n",
        "        Compute P(C_j), mean_j, var_j\n",
        "    '''\n",
        "    gmm_params = []\n",
        "    ...\n",
        "    return gmm_params\n",
        "\n",
        "\n",
        "def compute_probs(k, dataset, gmm_params):\n",
        "    '''\n",
        "        For all x_i in dataset, compute P(C_j | X_i)\n",
        "        = P(X_i | C_j)P(C_j) / P(X_i) for all C_j\n",
        "        return the list of lists of all P(C_j | X_i)\n",
        "        for all x_i in dataset.\n",
        "    '''\n",
        "    probs = []\n",
        "    ...\n",
        "    return probs\n",
        "\n",
        "\n",
        "def expectation_maximization(k, dataset, iterations):\n",
        "    '''\n",
        "        Repeat for a set number of iterations.\n",
        "    '''\n",
        "    gmm_params = gmm_init(k, dataset)\n",
        "    for _ in range(iterations):\n",
        "        # expectation step\n",
        "        probs = compute_probs(k, dataset, gmm_params)\n",
        "\n",
        "        # maximization step\n",
        "        gmm_params = compute_gmm(k, dataset, probs)\n",
        "\n",
        "    return probs, gmm_params"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes:\n",
        "\n",
        "1. your code should work with any number of components, each with reasonable parameters.\n",
        "2. your code should work for 1 to about 5 iterations of the EM algorithm. It may not work for iterations over 10 because the math we are doing may overflow and create `nans` - that's ok / don't worry about it."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
